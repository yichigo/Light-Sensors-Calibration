{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ad7483eae430>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpysolar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "#from netCDF4 import Dataset, num2date\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler,Normalizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import adjusted_rand_score, r2_score\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.colors\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from pysolar.solar import *\n",
    "import pytz\n",
    "import shap\n",
    "import pickle\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "n_jobs = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheap_node_list = ['001e063059c2', '001e06305a61', '001e06305a6c', '001e06318cd1',\n",
    "                   '001e06323a05', '001e06305a57', '001e06305a6b', '001e06318c28',\n",
    "                   '001e063239e3', '001e06323a12']\n",
    "\n",
    "cheap_node_id = '001e06305a61'#'001e06305a6b'\n",
    "node_id = '10004098'\n",
    "gps_node_id = '001e0610c2e9'\n",
    "dir_out = '../figures/' + cheap_node_id + '/'\n",
    "dir_data = '../data/'\n",
    "\n",
    "years = ['2019','2020'] ####\n",
    "months = ['1','2','3','4','5','6','7','8','9','10','11','12']\n",
    "days = np.array(range(1,31+1)).astype(str) #### np.array(range(1,31+1)).astype(str)\n",
    "days = list(days)\n",
    "\n",
    "hours = (np.array(range(0,24))).astype(str)\n",
    "hours = list(hours)\n",
    "\n",
    "bins = np.array(range(0,420+1)).astype(str)\n",
    "bins = list(bins)\n",
    "for i in range(len(bins)):\n",
    "    bins[i] = 'Spectrum[' + bins[i] + ']'\n",
    "\n",
    "wavelengths = np.array(range(360,780+1))#.astype(str)\n",
    "#for i in range(len(wavelengths)):\n",
    "#    wavelengths[i] = wavelengths[i] + 'nm'\n",
    "#wavelengths = list(wavelengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if data has been preprocessed before, run this directly\n",
    "fn_data = dir_data + node_id + '_'+ cheap_node_id +'.csv'\n",
    "df = pd.read_csv(fn_data, parse_dates=True, index_col = 'UTC')\n",
    "df = df[(df.index.date != datetime.date(2019, 12, 31)) # Minolta was covered in these dates\n",
    "       &(df.index.date != datetime.date(2019, 12, 27))\n",
    "       &(df.index.date != datetime.date(2020,  1,  1))\n",
    "       &(df.index.date != datetime.date(2020,  1,  2))]\n",
    "#        &(df.index.date != datetime.date(2020, 2, 14))\n",
    "#        &(df.index.date != datetime.date(2020, 2, 21))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['altitude'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [#'cloudPecentage', 'allRed', 'allGreen', 'allBlue',\n",
    "            #'skyRed', 'skyGreen', 'skyBlue', 'cloudRed', 'cloudGreen', 'cloudBlue',\n",
    "            'Violet', 'Blue', 'Green', 'Yellow', 'Orange', 'Red',\n",
    "            'Temperature', 'Pressure', 'Humidity',\n",
    "            #'Latitude', 'Longitude', 'Altitude',\n",
    "            #'NH3', 'CO', 'NO2', 'C3H8', 'C4H10', 'CH4', 'H2', 'C2H5OH', 'CO2',\n",
    "            'Luminosity', 'IR', 'Full', 'Visible', 'Lux',\n",
    "            'UVA', 'UVB', 'Visible Compensation','IR Compensation', 'UV Index',\n",
    "            'Zenith','Azimuth','Sun Distance']\n",
    "features = np.array(features)\n",
    "num_features = len(features)\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = df.columns[-421-1:-1].values # skip Illuminance, keep Wavelengths\n",
    "print(features)\n",
    "print(targets[[0,-1]])\n",
    "\n",
    "X = df[features]\n",
    "Y = df[targets] # MLP and scaler use multi output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data  For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# scale the data\n",
    "scaler_x = StandardScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "X_test_scaled = scaler_x.transform(X_test)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "Y_train_scaled = scaler_y.fit_transform(Y_train)\n",
    "Y_test_scaled = scaler_y.transform(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 18\n",
    "\n",
    "DR = 'PCA'\n",
    "Cluster = 'None'\n",
    "pca = PCA(n_components=n_components, random_state = RANDOM_STATE)\n",
    "\n",
    "X_train_scaled_DR = pca.fit_transform(X_train_scaled)\n",
    "X_test_scaled_DR = pca.transform(X_test_scaled)\n",
    "\n",
    "X_train_scaled_DR = pd.DataFrame(X_train_scaled_DR)\n",
    "X_test_scaled_DR = pd.DataFrame(X_test_scaled_DR)\n",
    "print(pca.explained_variance_)\n",
    "\n",
    "# save pca model\n",
    "dir_DR = '../models/' + cheap_node_id + '/'\n",
    "if not os.path.exists(dir_DR):\n",
    "    os.mkdir(dir_DR)\n",
    "\n",
    "fn_DR = dir_DR + DR + '.sav'\n",
    "pickle.dump(pca, open(fn_DR, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x2 = StandardScaler()\n",
    "X_train_scaled_DR_scaled = scaler_x2.fit_transform(X_train_scaled_DR)\n",
    "X_test_scaled_DR_scaled = scaler_x2.transform(X_test_scaled_DR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Start from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for Whole Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hidden_layer_sizes in [(128,128),\n",
    "#                            (64,64,64,64),\n",
    "#                            (32,32,32,32,32),\n",
    "#                            (512),\n",
    "#                            (256),\n",
    "#                            (256,256)\n",
    "#                           ]:\n",
    "#     print(hidden_layer_sizes)\n",
    "#     activation ='relu'\n",
    "#     solver = 'adam'\n",
    "#     alpha=1e-5 # L2 penalty (regularization term) parameter\n",
    "#     learning_rate = 'constant'\n",
    "\n",
    "#     # include layer structure and activation function\n",
    "#     structure = '_' + DR + str(n_components) + \\\n",
    "#                 '_' + str(hidden_layer_sizes)[1:-1].replace(', ','_') + \\\n",
    "#                 '_' + activation\n",
    "    \n",
    "#     start_time = time.time()\n",
    "#     regr = MLPRegressor(random_state=RANDOM_STATE,\n",
    "#                         hidden_layer_sizes=hidden_layer_sizes,\n",
    "#                         activation = activation,\n",
    "#                         solver = solver,\n",
    "#                         alpha = alpha,\n",
    "#                         learning_rate = learning_rate,\n",
    "#                         verbose = False\n",
    "#                         )\n",
    "#     regr.fit(X_train_scaled_DR_scaled, Y_train_scaled)\n",
    "\n",
    "#     print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "#     # save model\n",
    "#     dir_model = '../models/' + cheap_node_id + '/'\n",
    "#     if not os.path.exists(dir_model):\n",
    "#         os.mkdir(dir_model)\n",
    "#     dir_model += 'whole/'\n",
    "#     if not os.path.exists(dir_model):\n",
    "#         os.mkdir(dir_model)\n",
    "\n",
    "#     fn_model = dir_model + 'MLP_whole' + structure + '.sav'\n",
    "#     pickle.dump(regr, open(fn_model, 'wb'))\n",
    "    \n",
    "    \n",
    "#     # plot 10 % of the data\n",
    "#     Y_train_pred = scaler_y.inverse_transform(\n",
    "#                         regr.predict( X_train_scaled_DR_scaled )\n",
    "#                         ) # for train\n",
    "#     Y_test_pred = scaler_y.inverse_transform(\n",
    "#                         regr.predict( X_test_scaled_DR_scaled )\n",
    "#                         )# for image\n",
    "#     #Y_test_pred = regr.predict(X_test) # for test score\n",
    "\n",
    "#     train_score =  r2_score(Y_train, Y_train_pred)\n",
    "#     test_score = r2_score(Y_test, Y_test_pred)\n",
    "\n",
    "#     y_min = np.amin(Y_train.values)\n",
    "#     y_max = np.amax(Y_train.values)\n",
    "#     y_line = np.linspace(y_min,y_max,100)\n",
    "\n",
    "#     plt.rcParams[\"figure.figsize\"] = (8, 8) # (w, h)\n",
    "#     plt.rcParams.update({'font.size': 20})\n",
    "#     fig, ax = plt.subplots()\n",
    "#     plt.plot(y_line,y_line, '-k', label='y=x')\n",
    "\n",
    "#     length_sample = len(Y_test)//10\n",
    "#     plt.scatter(Y_train[:length_sample],Y_train_pred[:length_sample], s=1, c = 'blue',label = 'Train, R$^{2}$ ='+str(train_score)[:6])\n",
    "#     plt.scatter(Y_test[:length_sample],Y_test_pred[:length_sample], s=1, c = 'red', label = 'Test, R$^{2}$ ='+str(test_score)[:6])\n",
    "#     plt.xlim((y_min,y_max))\n",
    "#     plt.ylim((y_min,y_max))\n",
    "#     ax.set_title('Predicted vs Actual for Whole Spectrum')\n",
    "#     ax.set_xlabel('Actual Value')\n",
    "#     ax.set_ylabel('Predicted Value')\n",
    "#     plt.legend( loc='lower right')\n",
    "#     plt.grid()\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     if not os.path.exists(dir_out + 'whole'):\n",
    "#         os.mkdir(dir_out + 'whole')\n",
    "#     plt.savefig(dir_out + 'whole' +'/MLP_performance_whole'+structure+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes=(64,128,256)\n",
    "#hidden_layer_sizes=(128,128,128,128)\n",
    "#hidden_layer_sizes=(512, 512, 256, 256)\n",
    "#hidden_layer_sizes=(128,128,128,128,128,128)\n",
    "#hidden_layer_sizes=(128,128,128,128,128)\n",
    "\n",
    "activation ='relu'\n",
    "solver = 'adam'\n",
    "alpha=1e-5 # L2 penalty (regularization term) parameter, default 1e-5\n",
    "learning_rate = 'constant'\n",
    "\n",
    "# include layer structure and activation function\n",
    "structure = '_' + DR + str(n_components) + \\\n",
    "            '_' + str(hidden_layer_sizes)[1:-1].replace(', ','_') + \\\n",
    "            '_' + activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "regr = MLPRegressor(random_state = RANDOM_STATE,\n",
    "                    hidden_layer_sizes = hidden_layer_sizes,\n",
    "                    activation = activation,\n",
    "                    solver = solver,\n",
    "                    alpha = alpha,\n",
    "                    learning_rate = learning_rate,\n",
    "                    verbose = True\n",
    "                    )\n",
    "regr.fit(X_train_scaled_DR_scaled, Y_train_scaled)\n",
    "\n",
    "# fine tune the model\n",
    "regr.warm_start = True\n",
    "regr.learning_rate_init /= 10 # default 0.001\n",
    "regr.fit(X_train_scaled_DR_scaled, Y_train_scaled)\n",
    "\n",
    "regr.learning_rate_init /= 10 # default 0.001\n",
    "regr.fit(X_train_scaled_DR_scaled, Y_train_scaled)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "dir_model = '../models/' + cheap_node_id + '/'\n",
    "if not os.path.exists(dir_model):\n",
    "    os.mkdir(dir_model)\n",
    "dir_model += 'whole/'\n",
    "if not os.path.exists(dir_model):\n",
    "    os.mkdir(dir_model)\n",
    "\n",
    "fn_model = dir_model + 'MLP_whole' + structure + '.sav'\n",
    "pickle.dump(regr, open(fn_model, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_model = '../models/' + cheap_node_id + '/'  + 'whole/'\n",
    "fn_model = dir_model + 'MLP_whole' + structure + '.sav'\n",
    "regr = pickle.load(open(fn_model, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "Y_train_pred = scaler_y.inverse_transform(\n",
    "                    regr.predict( X_train_scaled_DR_scaled )\n",
    "                    ) # for train\n",
    "train_score = r2_score(Y_train, Y_train_pred)\n",
    "\n",
    "Y_test_pred = scaler_y.inverse_transform(\n",
    "                    regr.predict( X_test_scaled_DR_scaled )\n",
    "                    )# for test\n",
    "test_score = r2_score(Y_test, Y_test_pred)\n",
    "\n",
    "# flatten\n",
    "Y_train_flatten = Y_train.values.flatten()\n",
    "Y_train_pred_flatten = Y_train_pred.flatten()\n",
    "\n",
    "Y_test_flatten = Y_test.values.flatten()\n",
    "Y_test_pred_flatten = Y_test_pred.flatten()\n",
    "\n",
    "Y_min = np.min(Y_train_flatten)\n",
    "Y_max = np.mean(Y_train_flatten) + 5*np.std(Y_train_flatten)\n",
    "\n",
    "y_line = np.linspace(Y_min,Y_max,100)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8) # (w, h)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(y_line,y_line, '-k', label='y=x')\n",
    "\n",
    "id_train = np.random.choice(len(Y_train_flatten), 5000, replace = False)\n",
    "plt.scatter(Y_train_flatten[id_train],Y_train_pred_flatten[id_train], s=3, c = 'blue',label = 'Train, R$^{2}$ ='+str(train_score)[:6])\n",
    "id_test = np.random.choice(len(Y_test_flatten), 5000, replace = False)\n",
    "plt.scatter(Y_test_flatten[id_test],Y_test_pred_flatten[id_test], s=3, c = 'red', label = 'Test, R$^{2}$ ='+str(test_score)[:6])\n",
    "plt.xlim(Y_min, Y_max)\n",
    "plt.ylim(Y_min, Y_max)\n",
    "\n",
    "ax.set_title('Predicted vs Actual for Whole Spectrum')\n",
    "ax.set_xlabel('Actual Value')\n",
    "ax.set_ylabel('Predicted Value')\n",
    "plt.legend( loc='lower right')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "\n",
    "if not os.path.exists(dir_out + 'whole'):\n",
    "    os.mkdir(dir_out + 'whole')\n",
    "plt.savefig(dir_out + 'whole' +'/MLP_performance_whole'+structure+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bads = {}\n",
    "bads_var = {}\n",
    "counts = []\n",
    "for i in range(len(targets)):\n",
    "    var = targets[i]\n",
    "    temp = Y_train[var][ (Y_train[var]/Y_train_pred[:,i] > 1.2 ) & (Y_train[var]>0.05)]\n",
    "    # count bad dates\n",
    "    for date in temp.index.date:\n",
    "        if date in bads:\n",
    "            bads[date] += 1\n",
    "        else:\n",
    "            bads[date] = 1\n",
    "    # count bad wavelengths\n",
    "    var_i = int(var[:-2])\n",
    "    \n",
    "    bads_var[var_i] = len(temp)\n",
    "    counts.append(len(temp))\n",
    "        \n",
    "\n",
    "display(bads)\n",
    "\n",
    "\n",
    "plt.plot(wavelengths, counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "for i in range(len(wavelengths)):\n",
    "    train_scores.append( r2_score(Y_train.iloc[:,i], Y_train_pred[:,i]) )\n",
    "    test_scores.append( r2_score(Y_test.iloc[:,i], Y_test_pred[:,i]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10, 5) # (w, h)\n",
    "plt.plot(np.array(range(360,780+1)),train_scores, 'b', label = 'Train')\n",
    "plt.plot(np.array(range(360,780+1)),test_scores, 'r', label = 'Test')\n",
    "plt.ylim(0.995,1)\n",
    "plt.title(\"R$^2$ Distribution\")\n",
    "plt.legend( loc='lower right')\n",
    "plt.savefig(dir_out + 'whole' + '/MLP_R2' + structure + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object that can calculate shap values\n",
    "num_shap = 100\n",
    "explainer = shap.KernelExplainer(regr.predict, X_train_scaled_DR_scaled[:num_shap])\n",
    "# Calculate shap_values\n",
    "shap_values_DR_multi = explainer.shap_values(X_train_scaled_DR_scaled[:num_shap])\n",
    "shap_values_DR = np.mean(shap_values_DR_multi, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "max_display = n_components\n",
    "shap.summary_plot(shap_values_DR, X_train_scaled_DR_scaled[:num_shap],\n",
    "                  plot_size=(8,max_display/3.5),#'auto'\n",
    "                  max_display = max_display,\n",
    "                  show=False,\n",
    "                 )\n",
    "plt.tight_layout()\n",
    "plt.savefig(dir_out + 'whole' + '/MLP_shap' + DR + '_whole' + structure + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "max_display = num_features\n",
    "shap_values = pca.inverse_transform(shap_values_DR)\n",
    "shap.summary_plot(shap_values, X_train_scaled[:num_shap],\n",
    "                  feature_names = features,\n",
    "                  plot_size=(8,max_display/3.5),#'auto'\n",
    "                  max_display = max_display,\n",
    "                  show=False\n",
    "                 )\n",
    "plt.tight_layout()\n",
    "plt.savefig(dir_out + 'whole' + '/MLP_shap_'+ 'whole' +structure+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_DR = np.mean(np.abs(shap_values_DR), axis = 0)\n",
    "indices = np.argsort(importances_DR)\n",
    "\n",
    "# color positive/negative impact blue/red\n",
    "colormap = {1:'blue',-1:'red', 0:'grey'}\n",
    "sign = np.array([])\n",
    "for i in range(n_components):\n",
    "    sign_i = np.sign(np.corrcoef(X_train_scaled_DR_scaled[:,i], Y_train_scaled.mean(axis = 1))[0,1])\n",
    "    sign = np.append(sign, sign_i)\n",
    "\n",
    "sign = sign[indices]\n",
    "color = [colormap[x] for x in sign]\n",
    "\n",
    "shap.summary_plot(shap_values_DR, X_train_scaled_DR_scaled[:num_shap],\n",
    "                  plot_size=(8,max_display/3.5),#'auto'\n",
    "                  color = color,\n",
    "                  max_display = max_display,\n",
    "                  show = False,\n",
    "                  plot_type = 'bar'\n",
    "                 )\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.title(DR + ' Feature Importances for Whole Spectrum', fontsize = 20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(dir_out + 'whole' + '/MLP_'+DR+'Importances_' + 'whole' + structure +'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importances = np.mean(np.abs(shap_values), axis = 0)\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# color positive/negative impact blue/red\n",
    "colormap = {1:'blue',-1:'red', 0:'grey'}\n",
    "sign = np.array([])\n",
    "for i in range(num_features):\n",
    "    sign_i = np.sign(np.corrcoef(X_train_scaled[:,i], Y_train_scaled.mean(axis = 1))[0,1])\n",
    "    sign = np.append(sign, sign_i)\n",
    "\n",
    "sign = sign[indices]\n",
    "color = [colormap[x] for x in sign]\n",
    "\n",
    "shap.summary_plot(shap_values, X_train_scaled[:num_shap],\n",
    "                  feature_names = features,\n",
    "                  plot_size=(8,max_display/3.5),#'auto'\n",
    "                  color = color,\n",
    "                  max_display = max_display,\n",
    "                  show = False,\n",
    "                  plot_type = 'bar'\n",
    "                 )\n",
    "\n",
    "#plt.xscale(\"log\")\n",
    "plt.title('Feature Importances for Whole Spectrum', fontsize = 20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(dir_out + 'whole' + '/MLP_Importances_' + 'whole' + structure +'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Full Spectrum with Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[X_test.index.date == datetime.date(2020, 3, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wavelength_to_rgb import wavelength_to_rgb\n",
    "clim=(350,780)\n",
    "norm = plt.Normalize(*clim)\n",
    "wl = np.arange(clim[0],clim[1]+1,2)\n",
    "colorlist = list(zip(norm(wl),[wavelength_to_rgb(w) for w in wl]))\n",
    "spectralmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"spectrum\", colorlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "month = 3\n",
    "day = 29\n",
    "hour = 19\n",
    "minute = 32\n",
    "second = 40\n",
    "lag = 6\n",
    "date_string = '%04d-%02d-%02d %02d:%02d:%02d' % (year, month, day, hour, minute, second)\n",
    "date_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelengths = np.array(range(360,780+1))\n",
    "\n",
    "iwant = (X_test.index== date_string)\n",
    "X1 = X_test_scaled_DR_scaled[iwant]\n",
    "Y1 = Y_test[iwant].values\n",
    "Y1_pred = scaler_y.inverse_transform(regr.predict(X1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(16, 8))\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "spectrum = Y1[0]\n",
    "h = ax.plot(wavelengths, spectrum)\n",
    "\n",
    "plt.plot(wavelengths, spectrum)\n",
    "\n",
    "y2 = np.linspace(0, np.max(spectrum)*1.1, 100)\n",
    "\n",
    "XX,YY = np.meshgrid(wavelengths, y2)\n",
    "XX[XX<400] = 400\n",
    "extent=(np.min(wavelengths), np.max(wavelengths), np.min(y2), np.max(y2))\n",
    "\n",
    "plt.imshow(XX, clim=(350,780),  extent=extent, cmap=spectralmap, aspect='auto')\n",
    "plt.fill_between(wavelengths, spectrum, np.max(spectrum)*1.1, color='w')\n",
    "\n",
    "ax.set_title('Spectrum: %02d/%02d/%02d %02d:%02d:%02d' % (year, month, day, hour, minute, second))\n",
    "ax.set_xlabel('Wavelength / nm')\n",
    "ax.set_ylabel('Intensity')\n",
    "\n",
    "fig.savefig(dir_out+'Spectrum_%02d_%02d_%02d_%02d_%02d_%02d.png' % (year, month, day, hour, minute, second))\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout=True, figsize=(16, 8))\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "spectrum = Y1_pred[0]\n",
    "h = ax.plot(wavelengths, spectrum)\n",
    "\n",
    "plt.plot(wavelengths, spectrum)\n",
    "\n",
    "y2 = np.linspace(0, np.max(spectrum)*1.1, 100)\n",
    "\n",
    "XX,YY = np.meshgrid(wavelengths, y2)\n",
    "XX[XX<400] = 400\n",
    "extent=(np.min(wavelengths), np.max(wavelengths), np.min(y2), np.max(y2))\n",
    "\n",
    "plt.imshow(XX, clim=(350,780),  extent=extent, cmap=spectralmap, aspect='auto')\n",
    "plt.fill_between(wavelengths, spectrum, np.max(spectrum)*1.1, color='w')\n",
    "ax.set_title('Predicted Spectrum: %02d/%02d/%02d %02d:%02d:%02d' % (year, month, day, hour, minute, second))\n",
    "ax.set_xlabel('Wavelength / nm')\n",
    "ax.set_ylabel('Intensity')\n",
    "\n",
    "fig.savefig(dir_out+'Spectrum_%02d_%02d_%02d_%02d_%02d_%02d_pred.png' % (year, month, day, hour, minute, second))\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Illuminance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "print(mutual_info_regression(X[:10000],y[:10000]))\n",
    "print(time.time() - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.random.rand(100000,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "print(mutual_info_regression(X, df['Illuminance'], n_neighbors=3))\n",
    "print(time.time()-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "print(mutual_info_regression(X, df['Illuminance'], n_neighbors=30))\n",
    "print(time.time()-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "print(mutual_info_regression(xx, xx[:,1], n_neighbors=30))\n",
    "print(time.time()-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "print(mutual_info_regression(xx, xx[:,1], n_neighbors=300))\n",
    "print(time.time()-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
